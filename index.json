[{"authors":null,"categories":null,"content":"I am a post-doctoral researcher at CNRS in the ENSAE Fairplay team. I received my PhD in computer science from the University of Lille, where I worked under the direction of Emilie Kaufmann and Odalric-Ambrym Maillard in the INRIA ScooL team.\nDuring my PhD, I explored non-parametric algorithms for the Multi-Armed Bandit problem. Motivated by an application in agriculture, I investigated several approaches based on sub-sampling or Dirichlet Sampling, that work under realistic non-parametric assumptions on the reward distributions. I then considered some generalizations of these approaches motivated by practical considerations: risk-awareness by defining the objective with an alternative metric to the expected reward, non-stationarity, or batched feedback.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dorian-baudry.netlify.app/author/dorian-baudry/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dorian-baudry/","section":"authors","summary":"I am a post-doctoral researcher at CNRS in the ENSAE Fairplay team. I received my PhD in computer science from the University of Lille, where I worked under the direction of Emilie Kaufmann and Odalric-Ambrym Maillard in the INRIA ScooL team.","tags":null,"title":"Dorian Baudry","type":"authors"},{"authors":["Dorian Baudry","Yoan Russac","Emilie Kaufmann"],"categories":null,"content":"","date":1648379520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648379520,"objectID":"b10ff7aae99ab7992ace3245f8aeeea9","permalink":"https://dorian-baudry.netlify.app/publication/extreme/","publishdate":"2022-03-27T12:12:00+01:00","relpermalink":"/publication/extreme/","section":"publication","summary":"In this paper, we contribute to the Extreme Bandit problem, a variant of Multi-Armed Bandits in which the learner seeks to collect the largest possible reward. We first study the concentration of the maximum of i.i.d random variables under mild assumptions on the tail of the rewards distributions. This analysis motivates the introduction of Quantile of Maxima (QoMax). The properties of QoMax are sufficient to build an Explore-Then-Commit (ETC) strategy, QoMax-ETC, achieving strong asymptotic guarantees despite its simplicity. We then propose and analyze a more adaptive, anytime algorithm, QoMax-SDA, which combines QoMax with a subsampling method recently introduced by Baudry et al. (2021). Both algorithms are more efficient than existing approaches in two aspects (1) they lead to better empirical performance (2) they enjoy a significant reduction of the memory and time complexities.","tags":["Source Themes"],"title":"Efficient Algorithms for Extreme Bandits","type":"publication"},{"authors":["Dorian Baudry","Patrick Saux","Odalric-Ambrym Maillard"],"categories":null,"content":"","date":1638875520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638875520,"objectID":"442c1fe57f8d822d191eed1433256d2f","permalink":"https://dorian-baudry.netlify.app/publication/ds/","publishdate":"2021-12-07T12:12:00+01:00","relpermalink":"/publication/ds/","section":"publication","summary":"The stochastic multi-arm bandit problem has been extensively studied under standard assumptions on the arm's distribution (e.g bounded with known support, exponential family, etc). These assumptions are suitable for many real-world problems but sometimes they require knowledge (on tails for instance) that may not be precisely accessible to the practitioner, raising the question of the robustness of bandit algorithms to model misspecification. In this paper we study a generic Dirichlet Sampling (DS) algorithm, based on pairwise comparisons of empirical indices computed with re-sampling of the arms' observations and a data-dependent exploration bonus. We show that different variants of this strategy achieve provably optimal regret guarantees when the distributions are bounded and logarithmic regret for semi-bounded distributions with a mild quantile condition. We also show that a simple tuning achieve robustness with respect to a large class of unbounded distributions, at the cost of slightly worse than logarithmic asymptotic regret. We finally provide numerical experiments showing the merits of DS in a decision-making problem on synthetic agriculture data.","tags":["Source Themes"],"title":"From Optimality to Robustness: Adaptive Re-Sampling Strategies in Stochastic Bandits","type":"publication"},{"authors":["Dorian Baudry","Yoan Russac","Olivier Capp√©"],"categories":null,"content":"","date":1612915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612915200,"objectID":"d7cdabccea55e0b7221a012bc47f52ce","permalink":"https://dorian-baudry.netlify.app/publication/lb_sda/","publishdate":"2021-02-10T00:00:00Z","relpermalink":"/publication/lb_sda/","section":"publication","summary":"There has been a recent surge of interest in nonparametric bandit algorithms based on subsampling. One drawback however of these approaches is the additional complexity required by random subsampling and the storage of the full history of rewards. Our first contribution is to show that a simple deterministic subsampling rule, proposed in the recent work of Baudry et al. (2020) under the name of ''last-block subsampling'', is asymptotically optimal in one-parameter exponential families. In addition, we prove that these guarantees also hold when limiting the algorithm memory to a polylogarithmic function of the time horizon. These findings open up new perspectives, in particular for non-stationary scenarios in which the arm distributions evolve over time. We propose a variant of the algorithm in which only the most recent observations are used for subsampling, achieving optimal regret guarantees under the assumption of a known number of abrupt changes. Extensive numerical simulations highlight the merits of this approach, particularly when the changes are not only affecting the means of the rewards.","tags":["Source Themes"],"title":"On Limited-Memory Subsampling Strategies for Bandits","type":"publication"},{"authors":["Dorian Baudry","Romain Gautron","Emilie Kaufmann","Odalric-Ambrym Maillard"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"dfa8ce08addb1394f159c35cfb4041b3","permalink":"https://dorian-baudry.netlify.app/publication/ts_cvar/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/publication/ts_cvar/","section":"publication","summary":"In this paper we study a multi-arm bandit problem in which the quality of each arm is measured by the Conditional Value at Risk (CVaR) at some level alpha of the reward distribution. While existing works in this setting mainly focus on Upper Confidence Bound algorithms, we introduce a new Thompson Sampling approach for CVaR bandits on bounded rewards that is flexible enough to solve a variety of problems grounded on physical resources. Building on a recent work by Riou \u0026 Honda (2020), we introduce B-CVTS for continuous bounded rewards and M-CVTS for multinomial distributions. On the theoretical side, we provide a non-trivial extension of their analysis that enables to theoretically bound their CVaR regret minimization performance. Strikingly, our results show that these strategies are the first to provably achieve asymptotic optimality in CVaR bandits, matching the corresponding asymptotic lower bounds for this setting. Further, we illustrate empirically the benefit of Thompson Sampling approaches both in a realistic environment simulating a use-case in agriculture and on various synthetic examples.","tags":["Source Themes"],"title":"Optimal Thompson Sampling strategies for support-aware CVaR bandits","type":"publication"},{"authors":["Dorian Baudry","Emilie Kaufmann","Odalric-Ambrym Maillard"],"categories":null,"content":"","date":1603365120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603365120,"objectID":"e3cf8ead8905255109df35f6d668cbb3","permalink":"https://dorian-baudry.netlify.app/publication/sub-sampling/","publishdate":"2020-10-22T12:12:00+01:00","relpermalink":"/publication/sub-sampling/","section":"publication","summary":"In this paper we propose the first multi-armed bandit algorithm based on re-sampling that achieves asymptotically optimal regret simultaneously for different families of arms (namely Bernoulli, Gaussian and Poisson distributions). Unlike Thompson Sampling which requires to specify a different prior to be optimal in each case, our proposal RB-SDA does not need any distribution-dependent tuning. RB-SDA belongs to the family of Sub-sampling Duelling Algorithms (SDA) which combines the sub-sampling idea first used by the BESA [1] and SSMC [2] algorithms with different sub-sampling schemes. In particular, RB-SDA uses Random Block sampling. We perform an experimental study assessing the flexibility and robustness of this promising novel approach for exploration in bandit models.","tags":["Source Themes"],"title":"Sub-Sampling Algorithms for Efficient Non-Parametric Bandit Exploration","type":"publication"}]